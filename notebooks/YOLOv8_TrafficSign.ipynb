{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239b6e1d",
   "metadata": {},
   "source": [
    "Vietnamese Traffic Sign Recognition with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85WtH3eSZwth",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23991,
     "status": "ok",
     "timestamp": 1757418003917,
     "user": {
      "displayName": "Ngoc Nguyen",
      "userId": "15812430445966000358"
     },
     "user_tz": -420
    },
    "id": "85WtH3eSZwth",
    "outputId": "dc996217-b885-4b60-c941-d8b45e5c33ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#Install YOLOv8 and Requirements\n",
    "!pip install ultralytics --quiet\n",
    "!pip install seaborn --quiet\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341ad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total number of classes: 25\n",
      "\n",
      "üîç Images per class:\n",
      "Class 0: 451 images\n",
      "Class 1: 4920 images\n",
      "Class 10: 2490 images\n",
      "Class 11: 1470 images\n",
      "Class 12: 2100 images\n",
      "Class 13: 510 images\n",
      "Class 14: 2850 images\n",
      "Class 15: 1140 images\n",
      "Class 16: 1020 images\n",
      "Class 17: 450 images\n",
      "Class 18: 1288 images\n",
      "Class 19: 780 images\n",
      "Class 2: 2610 images\n",
      "Class 20: 2280 images\n",
      "Class 21: 720 images\n",
      "Class 22: 390 images\n",
      "Class 23: 660 images\n",
      "Class 24: 450 images\n",
      "Class 3: 2670 images\n",
      "Class 4: 3750 images\n",
      "Class 5: 3510 images\n",
      "Class 6: 780 images\n",
      "Class 7: 2730 images\n",
      "Class 8: 2670 images\n",
      "Class 9: 2790 images\n",
      "\n",
      "üìà Total number of images: 45479\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set dataset path\n",
    "dataset_path = \"D:/2025/Vietnamese-Traffic-Sign/myData\"\n",
    "\n",
    "# Get list of classes and count images\n",
    "classes = sorted(os.listdir(dataset_path))\n",
    "class_counts = {}\n",
    "\n",
    "for class_name in classes:\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        num_images = len(os.listdir(class_path))\n",
    "        class_counts[class_name] = num_images\n",
    "\n",
    "# Print dataset summary\n",
    "print(f\"Total number of classes: {len(classes)}\")\n",
    "print(\"\\nImages per class:\")\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"Class {class_name}: {count} images\")\n",
    "\n",
    "total_images = sum(class_counts.values())\n",
    "print(f\"\\nTotal number of images: {total_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chia train/val/test 80/10/10\n",
    "os.makedirs(\"dataset/train\", exist_ok=True)\n",
    "os.makedirs(\"dataset/val\", exist_ok=True)\n",
    "os.makedirs(\"dataset/test\", exist_ok=True)\n",
    "\n",
    "for cls in classes:\n",
    "    img_dir = os.path.join(dataset_path, cls)\n",
    "    imgs = os.listdir(img_dir)\n",
    "    random.shuffle(imgs)\n",
    "\n",
    "    n = len(imgs)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val = int(0.1 * n)\n",
    "\n",
    "    splits = {\n",
    "        \"train\": imgs[:n_train],\n",
    "        \"val\": imgs[n_train:n_train+n_val],\n",
    "        \"test\": imgs[n_train+n_val:]\n",
    "    }\n",
    "\n",
    "    for split, img_list in splits.items():\n",
    "        os.makedirs(f\"dataset/{split}/{cls}\", exist_ok=True)\n",
    "        for img in img_list:\n",
    "            shutil.copy(os.path.join(img_dir, img), f\"dataset/{split}/{cls}/{img}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T·∫°o YAML config file cho YOLO\n",
    "# ===================================\n",
    "with open(\"traffic.yaml\", \"w\") as f:\n",
    "    f.write(f\"\"\"\n",
    "path: dataset\n",
    "train: train\n",
    "val: val\n",
    "test: test\n",
    "names: {classes}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307254a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.234 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.226  Python-3.13.3 torch-2.9.0+cu128 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=D:/2025/Yolo_Code/dataset, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8m-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train_with_last_eval3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=D:\\2025\\Yolo_Code\\runs\\classify\\train_with_last_eval3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "Overriding model.yaml nc=1000 with nc=25\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   2655744  ultralytics.nn.modules.conv.Conv             [384, 768, 3, 2]              \n",
      "  8                  -1  2   7084032  ultralytics.nn.modules.block.C2f             [768, 768, 2, True]           \n",
      "  9                  -1  1   1017625  ultralytics.nn.modules.head.Classify         [768, 25]                     \n",
      "YOLOv8m-cls summary: 80 layers, 15,804,361 parameters, 15,804,361 gradients, 41.9 GFLOPs\n",
      "Transferred 228/230 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 1.22.2 ms, read: 0.10.0 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 61.5Mit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 0.10.0 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\val... 4546 images, 1 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4546/4546 4.6Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\2025\\Yolo_Code\\dataset\\val\\14\\00047_00014.jpg: ignoring corrupt image/label: [Errno 22] Invalid argument\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 38 weight(decay=0.0), 39 weight(decay=0.0005), 39 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\train_with_last_eval3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       1/50      1.35G      2.084         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 13.3it/s 1:25<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 14.0it/s 5.1s0.1s\n",
      "                   all      0.813      0.983\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 8.66.6 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 61.1Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 29.7it/s 38.3s<0.0s\n",
      "                   all      0.818      0.986\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val4\u001b[0m\n",
      "Epoch 1/50 | Train Acc: 0.8179 | Val Acc: 0.8130 | Train Loss: 2.0840 | Val Loss: 0.6329\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       2/50      1.36G     0.4645         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 14.8it/s 1:17<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.8it/s 2.8s0.1s\n",
      "                   all      0.959      0.999\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 12.95.0 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 33.7Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 32.2it/s 35.3s<0.0s\n",
      "                   all      0.962      0.999\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val5\u001b[0m\n",
      "Epoch 2/50 | Train Acc: 0.9625 | Val Acc: 0.9593 | Train Loss: 0.4645 | Val Loss: 0.1292\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       3/50       1.5G     0.2464         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 16.2it/s 1:10<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.7it/s 2.8s0.1s\n",
      "                   all      0.974          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 15.66.2 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 42.5Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 32.2it/s 35.3s<0.1s\n",
      "                   all      0.975          1\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val6\u001b[0m\n",
      "Epoch 3/50 | Train Acc: 0.9753 | Val Acc: 0.9736 | Train Loss: 0.2464 | Val Loss: 0.0751\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       4/50      1.51G     0.1744         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 24.8it/s 2.9s0.0s\n",
      "                   all      0.985          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.35.7 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 32.7Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.6it/s 35.9s<0.0s\n",
      "                   all      0.988          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val7\u001b[0m\n",
      "Epoch 4/50 | Train Acc: 0.9878 | Val Acc: 0.9846 | Train Loss: 0.1744 | Val Loss: 0.0415\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       5/50      1.51G     0.1338         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.1it/s 1:15<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 23.0it/s 3.1s0.2s\n",
      "                   all      0.992          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.74.0 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 57.2Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.5it/s 36.1s<0.0s\n",
      "                   all      0.993          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val8\u001b[0m\n",
      "Epoch 5/50 | Train Acc: 0.9927 | Val Acc: 0.9916 | Train Loss: 0.1338 | Val Loss: 0.0254\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       6/50      1.51G     0.1126         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.4it/s 1:14<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.4it/s 2.8s0.1s\n",
      "                   all      0.992          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 12.88.3 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 54.8Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.4it/s 36.2s<0.1s\n",
      "                   all      0.996          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val9\u001b[0m\n",
      "Epoch 6/50 | Train Acc: 0.9958 | Val Acc: 0.9923 | Train Loss: 0.1126 | Val Loss: 0.0191\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       7/50      1.51G     0.1016         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.5it/s 1:14<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.2it/s 2.9s0.1s\n",
      "                   all      0.994          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 13.96.0 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 45.1Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.8s<0.0s\n",
      "                   all      0.997          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val10\u001b[0m\n",
      "Epoch 7/50 | Train Acc: 0.9974 | Val Acc: 0.9943 | Train Loss: 0.1016 | Val Loss: 0.0162\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       8/50      1.51G    0.09169         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.7it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 23.6it/s 3.0s0.0s\n",
      "                   all      0.996          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 6.34.4 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 58.0Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.8s<0.1s\n",
      "                   all      0.998          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val11\u001b[0m\n",
      "Epoch 8/50 | Train Acc: 0.9977 | Val Acc: 0.9958 | Train Loss: 0.0917 | Val Loss: 0.0118\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       9/50      1.51G    0.08522         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.7it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.5it/s 2.8s0.1s\n",
      "                   all      0.997          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 13.35.0 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 41.8Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.6it/s 36.0s<0.0s\n",
      "                   all      0.999          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val12\u001b[0m\n",
      "Epoch 9/50 | Train Acc: 0.9986 | Val Acc: 0.9967 | Train Loss: 0.0852 | Val Loss: 0.0106\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      10/50      1.51G    0.08418         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 24.9it/s 2.9s0.0s\n",
      "                   all      0.996          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 12.34.9 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 52.8Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.7it/s 35.8s<0.1s\n",
      "                   all      0.999          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val13\u001b[0m\n",
      "Epoch 10/50 | Train Acc: 0.9989 | Val Acc: 0.9965 | Train Loss: 0.0842 | Val Loss: 0.0085\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      11/50      1.51G     0.0752         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 24.3it/s 3.0s0.1s\n",
      "                   all      0.997          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 12.05.6 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 23.2Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.6it/s 36.0s<0.1s\n",
      "                   all      0.999          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val14\u001b[0m\n",
      "Epoch 11/50 | Train Acc: 0.9993 | Val Acc: 0.9969 | Train Loss: 0.0752 | Val Loss: 0.0076\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      12/50      1.51G     0.0728         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.9it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 24.6it/s 2.9s0.0s\n",
      "                   all      0.998          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 8.76.6 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 57.5Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.9it/s 35.7s<0.1s\n",
      "                   all      0.999          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val15\u001b[0m\n",
      "Epoch 12/50 | Train Acc: 0.9995 | Val Acc: 0.9982 | Train Loss: 0.0728 | Val Loss: 0.0050\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      13/50      1.51G    0.07039         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.3it/s 2.8s0.1s\n",
      "                   all      0.998          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 12.85.2 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 55.6Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.9it/s 35.6s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val16\u001b[0m\n",
      "Epoch 13/50 | Train Acc: 0.9996 | Val Acc: 0.9985 | Train Loss: 0.0704 | Val Loss: 0.0050\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      14/50      1.51G    0.06869         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.7it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.4it/s 2.8s0.1s\n",
      "                   all      0.999          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 10.46.5 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 52.5Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.5it/s 36.1s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val17\u001b[0m\n",
      "Epoch 14/50 | Train Acc: 0.9998 | Val Acc: 0.9991 | Train Loss: 0.0687 | Val Loss: 0.0039\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      15/50      1.51G    0.06872         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.9it/s 1:11<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.4it/s 2.8s0.1s\n",
      "                   all      0.998          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 13.75.1 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 49.1Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.8s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val18\u001b[0m\n",
      "Epoch 15/50 | Train Acc: 0.9997 | Val Acc: 0.9980 | Train Loss: 0.0687 | Val Loss: 0.0062\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      16/50      1.51G    0.06274         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 24.4it/s 2.9s0.0s\n",
      "                   all      0.999          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 9.35.6 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 55.7Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.7s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val19\u001b[0m\n",
      "Epoch 16/50 | Train Acc: 0.9999 | Val Acc: 0.9991 | Train Loss: 0.0627 | Val Loss: 0.0036\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      17/50      1.51G    0.05674         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.9it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.5it/s 2.8s0.1s\n",
      "                   all      0.998          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 9.57.3 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 54.6Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.7s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val20\u001b[0m\n",
      "Epoch 17/50 | Train Acc: 0.9999 | Val Acc: 0.9982 | Train Loss: 0.0567 | Val Loss: 0.0042\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      18/50      1.51G    0.06089         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.7it/s 1:13<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.4it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 2.41.7 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 46.2Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 32.0it/s 35.5s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val21\u001b[0m\n",
      "Epoch 18/50 | Train Acc: 0.9999 | Val Acc: 0.9996 | Train Loss: 0.0609 | Val Loss: 0.0023\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      19/50      1.51G    0.05678         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.4it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.64.9 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 49.6Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 30.6it/s 37.2s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val22\u001b[0m\n",
      "Epoch 19/50 | Train Acc: 0.9999 | Val Acc: 0.9998 | Train Loss: 0.0568 | Val Loss: 0.0016\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      20/50      1.51G    0.05942         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.7it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.4it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.1 ms, read: 7.97.7 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 56.3Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.7s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val23\u001b[0m\n",
      "Epoch 20/50 | Train Acc: 0.9998 | Val Acc: 0.9996 | Train Loss: 0.0594 | Val Loss: 0.0013\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      21/50      1.51G    0.05373         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.1it/s 2.9s0.0s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 8.53.1 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 54.0Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.8s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val24\u001b[0m\n",
      "Epoch 21/50 | Train Acc: 0.9999 | Val Acc: 0.9996 | Train Loss: 0.0537 | Val Loss: 0.0018\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      22/50      1.51G    0.05553         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.5it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.03.9 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 43.6Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.9it/s 35.6s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val25\u001b[0m\n",
      "Epoch 22/50 | Train Acc: 1.0000 | Val Acc: 0.9996 | Train Loss: 0.0555 | Val Loss: 0.0016\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      23/50      1.51G    0.05085         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.7it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.3it/s 2.9s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.54.3 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 36.8Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.8s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val26\u001b[0m\n",
      "Epoch 23/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0508 | Val Loss: 0.0031\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      24/50      1.51G    0.05109         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.6it/s 1:13<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.6it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 8.54.5 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 40.5Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.7s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val27\u001b[0m\n",
      "Epoch 24/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0511 | Val Loss: 0.0008\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      25/50      1.51G    0.05044         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 24.9it/s 2.9s0.0s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.75.9 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 42.1Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.5it/s 36.1s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val28\u001b[0m\n",
      "Epoch 25/50 | Train Acc: 1.0000 | Val Acc: 1.0000 | Train Loss: 0.0504 | Val Loss: 0.0007\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      26/50      1.51G    0.04922         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.6it/s 1:13<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.5it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.76.1 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 63.2Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.7s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val29\u001b[0m\n",
      "Epoch 26/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0492 | Val Loss: 0.0007\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      27/50      1.51G    0.04787         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.6it/s 1:13<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.3it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.75.4 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 58.7Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.8s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val30\u001b[0m\n",
      "Epoch 27/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0479 | Val Loss: 0.0007\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      28/50      1.51G    0.04709         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.7it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.0it/s 2.9s0.0s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 12.96.1 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 48.6Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.6it/s 36.0s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val31\u001b[0m\n",
      "Epoch 28/50 | Train Acc: 1.0000 | Val Acc: 1.0000 | Train Loss: 0.0471 | Val Loss: 0.0006\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      29/50      1.51G    0.04536         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 24.5it/s 2.9s0.0s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.83.1 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 45.0Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.7s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val32\u001b[0m\n",
      "Epoch 29/50 | Train Acc: 1.0000 | Val Acc: 1.0000 | Train Loss: 0.0454 | Val Loss: 0.0006\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      30/50      1.51G    0.04153         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.6it/s 1:13<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.4it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 9.45.9 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 39.5Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.7it/s 35.9s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val33\u001b[0m\n",
      "Epoch 30/50 | Train Acc: 1.0000 | Val Acc: 1.0000 | Train Loss: 0.0415 | Val Loss: 0.0005\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      31/50      1.51G    0.04266         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.7it/s 1:13<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.7it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.74.9 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 53.8Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.8s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val34\u001b[0m\n",
      "Epoch 31/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0427 | Val Loss: 0.0005\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      32/50      1.51G    0.04019         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.9it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.5it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 10.84.9 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 25.9Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.7it/s 35.9s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val35\u001b[0m\n",
      "Epoch 32/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0402 | Val Loss: 0.0005\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      33/50      1.51G    0.03708         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 24.7it/s 2.9s0.0s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 12.84.4 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 26.5Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.7it/s 35.8s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val36\u001b[0m\n",
      "Epoch 33/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0371 | Val Loss: 0.0004\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      34/50      1.51G    0.03692         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 24.3it/s 3.0s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 10.66.2 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 60.0Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 32.2it/s 35.3s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val37\u001b[0m\n",
      "Epoch 34/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0369 | Val Loss: 0.0004\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      35/50      1.51G    0.03534         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.7it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.8it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 13.12.9 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 36.7Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.8s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val38\u001b[0m\n",
      "Epoch 35/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0353 | Val Loss: 0.0004\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      36/50      1.51G    0.03335         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.5it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 8.53.3 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 54.7Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.6it/s 35.9s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val39\u001b[0m\n",
      "Epoch 36/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0333 | Val Loss: 0.0004\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      37/50      1.51G     0.0326         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.6it/s 1:13<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.4it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 13.85.4 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 34.6Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 32.0it/s 35.6s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val40\u001b[0m\n",
      "Epoch 37/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0326 | Val Loss: 0.0005\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      38/50      1.51G    0.03014         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.4it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 14.94.9 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 57.2Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.7it/s 35.9s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val41\u001b[0m\n",
      "Epoch 38/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0301 | Val Loss: 0.0004\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      39/50      1.51G    0.02681         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.5it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 7.95.9 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 38.1Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.7s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val42\u001b[0m\n",
      "Epoch 39/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0268 | Val Loss: 0.0004\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      40/50      1.51G    0.02788         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 24.7it/s 2.9s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.04.6 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 55.7Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.8it/s 35.8s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val43\u001b[0m\n",
      "Epoch 40/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0279 | Val Loss: 0.0004\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      41/50      1.51G    0.02644         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 14.5it/s 1:18<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 14.2it/s 5.1s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 9.03.5 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 49.4Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.5it/s 36.1s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val44\u001b[0m\n",
      "Epoch 41/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0264 | Val Loss: 0.0004\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      42/50      1.51G    0.02527         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.0it/s 2.9s0.0s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.48.2 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 52.0Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.7it/s 35.8s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val45\u001b[0m\n",
      "Epoch 42/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0253 | Val Loss: 0.0004\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      43/50      1.51G    0.02342         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 23.7it/s 3.0s0.0s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 12.65.6 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 61.9Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.9it/s 35.7s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val46\u001b[0m\n",
      "Epoch 43/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0234 | Val Loss: 0.0004\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      44/50      1.51G    0.02221         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.5it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.66.2 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 32.2Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.9it/s 35.6s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val47\u001b[0m\n",
      "Epoch 44/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0222 | Val Loss: 0.0004\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      45/50      1.51G    0.02208         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.7it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.4it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 10.74.2 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 58.2Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 32.0it/s 35.6s<0.0s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val48\u001b[0m\n",
      "Epoch 45/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0221 | Val Loss: 0.0003\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      46/50      1.51G    0.02259         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.6it/s 1:13<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.3it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 12.55.6 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 49.3Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.9it/s 35.7s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val49\u001b[0m\n",
      "Epoch 46/50 | Train Acc: 1.0000 | Val Acc: 0.9998 | Train Loss: 0.0226 | Val Loss: 0.0003\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      47/50      1.51G    0.02093         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.9it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 24.3it/s 3.0s0.0s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.55.5 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 49.9Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 32.0it/s 35.5s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val50\u001b[0m\n",
      "Epoch 47/50 | Train Acc: 1.0000 | Val Acc: 1.0000 | Train Loss: 0.0209 | Val Loss: 0.0003\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      48/50      1.51G    0.02062         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.8it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.5it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 11.25.1 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 26.0Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 32.0it/s 35.5s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val51\u001b[0m\n",
      "Epoch 48/50 | Train Acc: 1.0000 | Val Acc: 1.0000 | Train Loss: 0.0206 | Val Loss: 0.0003\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      49/50      1.51G    0.01744         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.7it/s 1:12<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.4it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 12.24.3 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 59.3Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 31.7it/s 35.9s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val52\u001b[0m\n",
      "Epoch 49/50 | Train Acc: 1.0000 | Val Acc: 1.0000 | Train Loss: 0.0174 | Val Loss: 0.0003\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      50/50      1.51G    0.01731         30        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 15.6it/s 1:13<0.1s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 24.5it/s 2.9s0.0s\n",
      "                   all          1          1\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 7.72.8 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 44.0Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 29.7it/s 38.3s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val53\u001b[0m\n",
      "Epoch 50/50 | Train Acc: 1.0000 | Val Acc: 1.0000 | Train Loss: 0.0173 | Val Loss: 0.0003\n",
      "\n",
      "50 epochs completed in 2.053 hours.\n",
      "Optimizer stripped from D:\\2025\\Yolo_Code\\runs\\classify\\train_with_last_eval3\\weights\\last.pt, 31.7MB\n",
      "Optimizer stripped from D:\\2025\\Yolo_Code\\runs\\classify\\train_with_last_eval3\\weights\\best.pt, 31.7MB\n",
      "\n",
      "Validating D:\\2025\\Yolo_Code\\runs\\classify\\train_with_last_eval3\\weights\\best.pt...\n",
      "Ultralytics 8.3.226  Python-3.13.3 torch-2.9.0+cu128 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72/72 25.4it/s 2.8s0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.5ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\train_with_last_eval3\u001b[0m\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 3.92.1 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\train... 36382 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 36382/36382 42.9Mit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1137/1137 30.2it/s 37.6s<0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val54\u001b[0m\n",
      "Epoch 50/50 | Train Acc: 1.0000 | Val Acc: 1.0000 | Train Loss: 0.0173 | Val Loss: 0.0003\n",
      "üíæ Saved CSV: D:\\2025\\Yolo_Code\\runs\\classify\\train_with_last_eval3\\results_with_train_acc.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Saved plot: D:\\2025\\Yolo_Code\\runs\\classify\\train_with_last_eval3\\metrics_train_val_4curves.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils import LOGGER\n",
    "\n",
    "# ======= C·∫•u h√¨nh =======\n",
    "YAML_DATA = \"D:/2025/Yolo_Code/dataset\"     # file data (train/val/test)\n",
    "EPOCHS    = 50\n",
    "IMGSZ     = 224\n",
    "BATCH     = 32\n",
    "WORKERS   = 8\n",
    "DEVICE_ID = 0 if torch.cuda.is_available() else None  # None -> CPU\n",
    "\n",
    "# H√†m l·∫•y top1 cho m·ªçi version Ultralytics\n",
    "def _get_top1(mets):\n",
    "    for k in (\"top1\", \"accuracy_top1\"):\n",
    "        if hasattr(mets, k):\n",
    "            return float(getattr(mets, k))\n",
    "    rd = getattr(mets, \"results_dict\", None)\n",
    "    if isinstance(rd, dict):\n",
    "        for k in (\"top1\", \"accuracy_top1\"):\n",
    "            if k in rd:\n",
    "                return float(rd[k])\n",
    "    raise RuntimeError(\"Kh√¥ng l·∫•y ƒë∆∞·ª£c top1 t·ª´ metrics.\")\n",
    "\n",
    "# ======= Load model v√† g·∫Øn callback =======\n",
    "model = YOLO(\"yolov8m-cls.pt\")\n",
    "\n",
    "def on_fit_epoch_end(trainer):\n",
    "    \n",
    "    try:\n",
    "        save_dir   = Path(trainer.save_dir)\n",
    "        weightsdir = save_dir / \"weights\"\n",
    "        last_ckpt  = weightsdir / \"last.pt\"\n",
    "        if not last_ckpt.exists():\n",
    "            LOGGER.warning(f\"[callback] Ch∆∞a th·∫•y {last_ckpt}, b·ªè qua epoch n√†y.\")\n",
    "            return\n",
    "\n",
    "        # ƒê√°nh gi√° train b·∫±ng evaluator tr√™n last.pt\n",
    "        m_wrap = YOLO(str(last_ckpt))\n",
    "        mets = m_wrap.val(\n",
    "            data=YAML_DATA, split=\"train\",\n",
    "            imgsz=IMGSZ, batch=BATCH,\n",
    "            device=(trainer.device if hasattr(trainer, \"device\") else DEVICE_ID),\n",
    "            verbose=False\n",
    "        )\n",
    "        train_acc = _get_top1(mets)\n",
    "\n",
    "        # Ghi v√†o results.csv\n",
    "        results_csv = save_dir / \"results.csv\"\n",
    "        df = pd.read_csv(results_csv)\n",
    "        if \"train/acc\" not in df.columns:\n",
    "            df[\"train/acc\"] = pd.NA\n",
    "        row_idx = min(trainer.epoch, len(df)-1)  # ph√≤ng l·ªách\n",
    "        df.loc[row_idx, \"train/acc\"] = train_acc\n",
    "        df.to_csv(results_csv, index=False)\n",
    "\n",
    "        LOGGER.info(f\"Epoch {trainer.epoch+1}/{trainer.epochs} | \"\n",
    "                    f\"Train Acc: {train_acc:.4f} | \"\n",
    "                    f\"Val Acc: {float(df.loc[row_idx, 'metrics/accuracy_top1']):.4f} | \"\n",
    "                    f\"Train Loss: {float(df.loc[row_idx, 'train/loss']):.4f} | \"\n",
    "                    f\"Val Loss: {float(df.loc[row_idx, 'val/loss']):.4f}\")\n",
    "    except Exception as e:\n",
    "        LOGGER.warning(f\"[callback] l·ªói log train acc: {e}\")\n",
    "\n",
    "# G·∫Øn callback\n",
    "model.add_callback(\"on_fit_epoch_end\", on_fit_epoch_end)\n",
    "\n",
    "# ======= Train =======\n",
    "train_results = model.train(\n",
    "    data=YAML_DATA,\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMGSZ,\n",
    "    batch=BATCH,\n",
    "    workers=WORKERS,\n",
    "    device=DEVICE_ID,\n",
    "    name=\"train_with_last_eval\",   # t√™n run\n",
    "    project=None                   # m·∫∑c ƒë·ªãnh runs/classify/...\n",
    ")\n",
    "\n",
    "# ======= ƒë·ªçc CSV & V·∫Ω bi·ªÉu ƒë·ªì (Loss & Acc cho Train/Val) =======\n",
    "save_dir    = Path(getattr(model, \"trainer\").save_dir)\n",
    "results_csv = save_dir / \"results.csv\"\n",
    "\n",
    "df = pd.read_csv(results_csv).copy()\n",
    "if \"train/acc\" not in df.columns:\n",
    "    \n",
    "    df[\"train/acc\"] = pd.NA\n",
    "\n",
    "# L∆∞u CSV th√™m c·ªôt train/acc\n",
    "merged_csv = save_dir / \"results_with_train_acc.csv\"\n",
    "df.to_csv(merged_csv, index=False)\n",
    "print(f\"Saved CSV: {merged_csv}\")\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì (Loss & Acc cho Train/Val)\n",
    "fig, ax1 = plt.subplots(figsize=(12,7))\n",
    "ax2 = ax1.twinx()\n",
    "l1, = ax1.plot(df.index+1, df[\"train/loss\"], label=\"Train Loss\")\n",
    "l2, = ax1.plot(df.index+1, df[\"val/loss\"],   label=\"Val Loss\")\n",
    "ax1.set_xlabel(\"Epoch\"); ax1.set_ylabel(\"Loss\"); ax1.grid(True, alpha=0.3)\n",
    "l3, = ax2.plot(df.index+1, df[\"train/acc\"].astype(float), label=\"Train Acc\", linestyle=\"--\")\n",
    "l4, = ax2.plot(df.index+1, df[\"metrics/accuracy_top1\"],   label=\"Val Acc\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax1.legend([l1,l2,l3,l4], [\"Train Loss\",\"Val Loss\",\"Train Acc\",\"Val Acc\"], loc=\"center right\")\n",
    "plt.title(\"YOLOv8 Classification ‚Äì Train/Val Loss & Acc (per-epoch via last.pt evaluator)\")\n",
    "plt.tight_layout()\n",
    "out_png = save_dir / \"metrics_train_val_4curves.png\"\n",
    "plt.savefig(out_png, dpi=150); plt.show()\n",
    "print(f\"Saved plot: {out_png}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 2.0840, Train Acc: 0.8179, Val Loss: 0.6329 ,Val Acc: 0.8130\n",
      "Epoch 2/50, Train Loss: 0.4645, Train Acc: 0.9625, Val Loss: 0.1292 ,Val Acc: 0.9593\n",
      "Epoch 3/50, Train Loss: 0.2464, Train Acc: 0.9753, Val Loss: 0.0751 ,Val Acc: 0.9736\n",
      "Epoch 4/50, Train Loss: 0.1744, Train Acc: 0.9878, Val Loss: 0.0415 ,Val Acc: 0.9846\n",
      "Epoch 5/50, Train Loss: 0.1338, Train Acc: 0.9927, Val Loss: 0.0254 ,Val Acc: 0.9916\n",
      "Epoch 6/50, Train Loss: 0.1126, Train Acc: 0.9958, Val Loss: 0.0191 ,Val Acc: 0.9923\n",
      "Epoch 7/50, Train Loss: 0.1016, Train Acc: 0.9974, Val Loss: 0.0162 ,Val Acc: 0.9943\n",
      "Epoch 8/50, Train Loss: 0.0917, Train Acc: 0.9977, Val Loss: 0.0118 ,Val Acc: 0.9958\n",
      "Epoch 9/50, Train Loss: 0.0852, Train Acc: 0.9986, Val Loss: 0.0106 ,Val Acc: 0.9967\n",
      "Epoch 10/50, Train Loss: 0.0842, Train Acc: 0.9989, Val Loss: 0.0085 ,Val Acc: 0.9965\n",
      "Epoch 11/50, Train Loss: 0.0752, Train Acc: 0.9993, Val Loss: 0.0076 ,Val Acc: 0.9969\n",
      "Epoch 12/50, Train Loss: 0.0728, Train Acc: 0.9995, Val Loss: 0.0050 ,Val Acc: 0.9982\n",
      "Epoch 13/50, Train Loss: 0.0704, Train Acc: 0.9996, Val Loss: 0.0050 ,Val Acc: 0.9985\n",
      "Epoch 14/50, Train Loss: 0.0687, Train Acc: 0.9998, Val Loss: 0.0039 ,Val Acc: 0.9991\n",
      "Epoch 15/50, Train Loss: 0.0687, Train Acc: 0.9997, Val Loss: 0.0062 ,Val Acc: 0.9980\n",
      "Epoch 16/50, Train Loss: 0.0627, Train Acc: 0.9999, Val Loss: 0.0036 ,Val Acc: 0.9991\n",
      "Epoch 17/50, Train Loss: 0.0567, Train Acc: 0.9999, Val Loss: 0.0042 ,Val Acc: 0.9982\n",
      "Epoch 18/50, Train Loss: 0.0609, Train Acc: 0.9999, Val Loss: 0.0023 ,Val Acc: 0.9996\n",
      "Epoch 19/50, Train Loss: 0.0568, Train Acc: 0.9999, Val Loss: 0.0016 ,Val Acc: 0.9998\n",
      "Epoch 20/50, Train Loss: 0.0594, Train Acc: 0.9998, Val Loss: 0.0013 ,Val Acc: 0.9996\n",
      "Epoch 21/50, Train Loss: 0.0537, Train Acc: 0.9999, Val Loss: 0.0018 ,Val Acc: 0.9996\n",
      "Epoch 22/50, Train Loss: 0.0555, Train Acc: 1.0000, Val Loss: 0.0016 ,Val Acc: 0.9996\n",
      "Epoch 23/50, Train Loss: 0.0508, Train Acc: 1.0000, Val Loss: 0.0031 ,Val Acc: 0.9998\n",
      "Epoch 24/50, Train Loss: 0.0511, Train Acc: 1.0000, Val Loss: 0.0008 ,Val Acc: 0.9998\n",
      "Epoch 25/50, Train Loss: 0.0504, Train Acc: 1.0000, Val Loss: 0.0007 ,Val Acc: 1.0000\n",
      "Epoch 26/50, Train Loss: 0.0492, Train Acc: 1.0000, Val Loss: 0.0007 ,Val Acc: 0.9998\n",
      "Epoch 27/50, Train Loss: 0.0479, Train Acc: 1.0000, Val Loss: 0.0007 ,Val Acc: 0.9998\n",
      "Epoch 28/50, Train Loss: 0.0471, Train Acc: 1.0000, Val Loss: 0.0006 ,Val Acc: 1.0000\n",
      "Epoch 29/50, Train Loss: 0.0454, Train Acc: 1.0000, Val Loss: 0.0006 ,Val Acc: 1.0000\n",
      "Epoch 30/50, Train Loss: 0.0415, Train Acc: 1.0000, Val Loss: 0.0005 ,Val Acc: 1.0000\n",
      "Epoch 31/50, Train Loss: 0.0427, Train Acc: 1.0000, Val Loss: 0.0005 ,Val Acc: 0.9998\n",
      "Epoch 32/50, Train Loss: 0.0402, Train Acc: 1.0000, Val Loss: 0.0005 ,Val Acc: 0.9998\n",
      "Epoch 33/50, Train Loss: 0.0371, Train Acc: 1.0000, Val Loss: 0.0004 ,Val Acc: 0.9998\n",
      "Epoch 34/50, Train Loss: 0.0369, Train Acc: 1.0000, Val Loss: 0.0004 ,Val Acc: 0.9998\n",
      "Epoch 35/50, Train Loss: 0.0353, Train Acc: 1.0000, Val Loss: 0.0004 ,Val Acc: 0.9998\n",
      "Epoch 36/50, Train Loss: 0.0333, Train Acc: 1.0000, Val Loss: 0.0004 ,Val Acc: 0.9998\n",
      "Epoch 37/50, Train Loss: 0.0326, Train Acc: 1.0000, Val Loss: 0.0005 ,Val Acc: 0.9998\n",
      "Epoch 38/50, Train Loss: 0.0301, Train Acc: 1.0000, Val Loss: 0.0004 ,Val Acc: 0.9998\n",
      "Epoch 39/50, Train Loss: 0.0268, Train Acc: 1.0000, Val Loss: 0.0004 ,Val Acc: 0.9998\n",
      "Epoch 40/50, Train Loss: 0.0279, Train Acc: 1.0000, Val Loss: 0.0004 ,Val Acc: 0.9998\n",
      "Epoch 41/50, Train Loss: 0.0264, Train Acc: 1.0000, Val Loss: 0.0004 ,Val Acc: 0.9998\n",
      "Epoch 42/50, Train Loss: 0.0253, Train Acc: 1.0000, Val Loss: 0.0004 ,Val Acc: 0.9998\n",
      "Epoch 43/50, Train Loss: 0.0234, Train Acc: 1.0000, Val Loss: 0.0004 ,Val Acc: 0.9998\n",
      "Epoch 44/50, Train Loss: 0.0222, Train Acc: 1.0000, Val Loss: 0.0004 ,Val Acc: 0.9998\n",
      "Epoch 45/50, Train Loss: 0.0221, Train Acc: 1.0000, Val Loss: 0.0003 ,Val Acc: 0.9998\n",
      "Epoch 46/50, Train Loss: 0.0226, Train Acc: 1.0000, Val Loss: 0.0003 ,Val Acc: 0.9998\n",
      "Epoch 47/50, Train Loss: 0.0209, Train Acc: 1.0000, Val Loss: 0.0003 ,Val Acc: 1.0000\n",
      "Epoch 48/50, Train Loss: 0.0206, Train Acc: 1.0000, Val Loss: 0.0003 ,Val Acc: 1.0000\n",
      "Epoch 49/50, Train Loss: 0.0174, Train Acc: 1.0000, Val Loss: 0.0003 ,Val Acc: 1.0000\n",
      "Epoch 50/50, Train Loss: 0.0173, Train Acc: 1.0000, Val Loss: 0.0003 ,Val Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path file CSV\n",
    "csv_path = 'D:/2025/Yolo_Code/runs/classify/train_with_last_eval3/results_with_train_acc.csv'\n",
    "\n",
    "# ƒê·ªçc file CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ki·ªÉm tra c√°c c·ªôt c·∫ßn thi·∫øt\n",
    "required_cols = ['epoch', 'train/loss', 'metrics/accuracy_top1', 'val/loss', 'train/acc']\n",
    "if all(col in df.columns for col in required_cols):\n",
    "    for _, row in df.iterrows():\n",
    "        print(f\"Epoch {int(row['epoch'])}/50, Train Loss: {row['train/loss']:.4f}, \"\n",
    "              f\"Train Acc: {row['train/acc']:.4f}, Val Loss: {row['val/loss']:.4f} ,Val Acc: {row['metrics/accuracy_top1']:.4f}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng ƒë·ªß c√°c c·ªôt c·∫ßn thi·∫øt trong file CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7dcUkRZwtj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1262,
     "status": "ok",
     "timestamp": 1757418033445,
     "user": {
      "displayName": "Ngoc Nguyen",
      "userId": "15812430445966000358"
     },
     "user_tz": -420
    },
    "id": "ea7dcUkRZwtj",
    "outputId": "d893dc35-db14-4c2e-8b71-37335f5271ca"
   },
   "outputs": [],
   "source": [
    "#Load YOLOv8 Model\n",
    "model = YOLO('yolov8m-cls.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LpMRRKJCZwtl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30200,
     "status": "ok",
     "timestamp": 1757422293548,
     "user": {
      "displayName": "Ngoc Nguyen",
      "userId": "15812430445966000358"
     },
     "user_tz": -420
    },
    "id": "LpMRRKJCZwtl",
    "outputId": "3c2914d1-c1ec-417c-8c34-283749ede54e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.226  Python-3.13.3 torch-2.9.0+cu128 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLOv8m-cls summary (fused): 42 layers, 15,794,681 parameters, 0 gradients, 41.7 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\train... found 36382 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\val... found 4547 images in 25 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m D:\\2025\\Yolo_Code\\dataset\\test... found 4550 images in 25 classes  \n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 3.21.8 MB/s, size: 0.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\2025\\Yolo_Code\\dataset\\val... 4546 images, 1 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4546/4546 7.5Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\2025\\Yolo_Code\\dataset\\val\\14\\00047_00014.jpg: ignoring corrupt image/label: [Errno 22] Invalid argument\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 285/285 52.3it/s 5.5s0.1s\n",
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\2025\\Yolo_Code\\runs\\classify\\val\u001b[0m\n",
      "üîç Evaluation Metrics: ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
      "\n",
      "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x00000243E97B0910>\n",
      "curves: []\n",
      "curves_results: []\n",
      "fitness: 1.0\n",
      "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
      "results_dict: {'metrics/accuracy_top1': 1.0, 'metrics/accuracy_top5': 1.0, 'fitness': 1.0}\n",
      "save_dir: WindowsPath('D:/2025/Yolo_Code/runs/classify/val')\n",
      "speed: {'preprocess': 0.07981445258352175, 'inference': 0.9461553672367926, 'loss': 0.0011093928183650609, 'postprocess': 0.0020104045922316044}\n",
      "task: 'classify'\n",
      "top1: 1.0\n",
      "top5: 1.0\n"
     ]
    }
   ],
   "source": [
    "# üß™ STEP 6: Validation\n",
    "metrics = model.val()\n",
    "print(\"Evaluation Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f7df9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Accuracy: 0.9996 (4548/4550)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ensure y_true / y_pred exist. If not, compute them using the test folder (same logic as CELL INDEX 18).\n",
    "if 'y_true' not in globals() or len(y_true) == 0 or 'y_pred' not in globals() or len(y_pred) == 0:\n",
    "\ty_true, y_pred = [], []\n",
    "\tval_dir = \"D:/2025/Yolo_Code/dataset/test\"\n",
    "\tclasses = sorted(os.listdir(val_dir))\n",
    "\n",
    "\tfor class_index, class_name in enumerate(classes):\n",
    "\t\tclass_folder = os.path.join(val_dir, class_name)\n",
    "\t\tif not os.path.isdir(class_folder):\n",
    "\t\t\tcontinue\n",
    "\t\tfor img_file in os.listdir(class_folder):\n",
    "\t\t\timg_path = os.path.join(class_folder, img_file)\n",
    "\t\t\ttry:\n",
    "\t\t\t\tresults = model(img_path, verbose=False)\n",
    "\t\t\t\tif len(results) == 0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tres = results[0]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Get probs robustly\n",
    "\t\t\t\tprobs = getattr(res, 'probs', None)\n",
    "\t\t\t\tif probs is None:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t# Extract the top1 prediction index\n",
    "\t\t\t\ttop1_idx = getattr(probs, 'top1', None)\n",
    "\t\t\t\tif top1_idx is None:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\n",
    "\t\t\t\tpred = int(top1_idx)\n",
    "\t\t\t\ty_pred.append(pred)\n",
    "\t\t\t\ty_true.append(class_index)\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\t# skip images that cause errors\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "# Validate lists\n",
    "if len(y_true) == 0:\n",
    "\traise RuntimeError(\"No predictions collected. Ensure the test dataset exists and the model is loaded (see CELL INDEX 18/20).\")\n",
    "\n",
    "# Compute and print accuracy\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "correct = sum(1 for a, b in zip(y_true, y_pred) if a == b)\n",
    "print(f\" Test Accuracy: {acc:.4f} ({correct}/{len(y_true)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yk1uLXiWZwtm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 267861,
     "status": "ok",
     "timestamp": 1757422719597,
     "user": {
      "displayName": "Ngoc Nguyen",
      "userId": "15812430445966000358"
     },
     "user_tz": -420
    },
    "id": "Yk1uLXiWZwtm",
    "outputId": "c75a2017-0fc5-4f1a-f88a-9dc1e95cfc30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        46\n",
      "           1     1.0000    1.0000    1.0000       492\n",
      "          10     1.0000    1.0000    1.0000       249\n",
      "          11     1.0000    1.0000    1.0000       147\n",
      "          12     1.0000    0.9905    0.9952       210\n",
      "          13     1.0000    1.0000    1.0000        51\n",
      "          14     1.0000    1.0000    1.0000       285\n",
      "          15     1.0000    1.0000    1.0000       114\n",
      "          16     1.0000    1.0000    1.0000       102\n",
      "          17     0.9783    1.0000    0.9890        45\n",
      "          18     1.0000    1.0000    1.0000       130\n",
      "          19     1.0000    1.0000    1.0000        78\n",
      "           2     1.0000    1.0000    1.0000       261\n",
      "          20     1.0000    1.0000    1.0000       228\n",
      "          21     1.0000    1.0000    1.0000        72\n",
      "          22     1.0000    1.0000    1.0000        39\n",
      "          23     1.0000    1.0000    1.0000        66\n",
      "          24     0.9783    1.0000    0.9890        45\n",
      "           3     1.0000    1.0000    1.0000       267\n",
      "           4     1.0000    1.0000    1.0000       375\n",
      "           5     1.0000    1.0000    1.0000       351\n",
      "           6     1.0000    1.0000    1.0000        78\n",
      "           7     1.0000    1.0000    1.0000       273\n",
      "           8     1.0000    1.0000    1.0000       267\n",
      "           9     1.0000    1.0000    1.0000       279\n",
      "\n",
      "    accuracy                         0.9996      4550\n",
      "   macro avg     0.9983    0.9996    0.9989      4550\n",
      "weighted avg     0.9996    0.9996    0.9996      4550\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classification Report & Confusion Matrix\n",
    "val_dir = \"D:/2025/Yolo_Code/dataset/test\"\n",
    "classes = sorted(os.listdir(val_dir))\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for class_index, class_name in enumerate(classes):\n",
    "    class_folder = os.path.join(val_dir, class_name)\n",
    "    for img_file in os.listdir(class_folder):\n",
    "        img_path = os.path.join(class_folder, img_file)\n",
    "        try:\n",
    "            results = model(img_path, verbose=False)[0]\n",
    "            pred_class = results.probs.top1\n",
    "            y_pred.append(pred_class)\n",
    "            y_true.append(class_index)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=classes, digits=4)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "with open(\"outputs/yolov8_classification_report.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.savefig(\"outputs/yolov8_confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DkjyIs6WZwto",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1757422794754,
     "user": {
      "displayName": "Ngoc Nguyen",
      "userId": "15812430445966000358"
     },
     "user_tz": -420
    },
    "id": "DkjyIs6WZwto",
    "outputId": "76923555-4a58-4fd1-9a52-4bce454a6ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Inference FPS: 27.02 frames/sec\n"
     ]
    }
   ],
   "source": [
    "# FPS Measurement\n",
    "test_images = [os.path.join(val_dir, classes[0], f) for f in os.listdir(os.path.join(val_dir, classes[0]))[:20]]\n",
    "start_time = time.time()\n",
    "for img in test_images:\n",
    "    model(img, verbose=False)\n",
    "end_time = time.time()\n",
    "fps = len(test_images) / (end_time - start_time)\n",
    "print(f\"‚ö° Inference FPS: {fps:.2f} frames/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527bbdc",
   "metadata": {},
   "source": [
    "API Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd11b5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: D:/2025/Vietnamese-Traffic-Sign/myData\n",
      "Found 25 class folders\n",
      "Loaded labels CSV with columns: ['ClassId', 'Name']\n",
      "Using id column: 'ClassId', name column: 'Name'\n",
      "Built display mapping for 50 entries (mixed int/str keys)\n",
      "Using weights: D:/2025/Yolo_Code/runs/yolov8_cls_vi_signs_gpu/weights/best.pt\n",
      "Running inference on: D:/2025/Test_Anh/50.jpg\n",
      "\n",
      "image 1/1 D:\\2025\\Test_Anh\\50.jpg: 224x224 2 1.00, 8 0.00, 7 0.00, 21 0.00, 0 0.00, 58.4ms\n",
      "Speed: 5.0ms preprocess, 58.4ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Number of classes (model output): 25\n",
      "\n",
      "Top predictions:\n",
      "1. Bien bao cam nguoc chieu (Index 12) ‚Äî Confidence: 0.9998 (99.98%)\n",
      "2. Bien bao giao nhau vong xuyen (Index 23) ‚Äî Confidence: 0.0000 (0.00%)\n",
      "3. Bien bao di thang hoac re trai (Index 22) ‚Äî Confidence: 0.0000 (0.00%)\n",
      "4. Bien bao dang bao tri duong bo (Index 14) ‚Äî Confidence: 0.0000 (0.00%)\n",
      "5. Bien bao gioi han toc do 20kmh (Index 0) ‚Äî Confidence: 0.0000 (0.00%)\n",
      "\n",
      "API-style output (dict):\n",
      "{'image': '50.jpg', 'top1': {'index': 12, 'folder_name': '2', 'display_name': 'Bien bao cam nguoc chieu', 'confidence': 0.9998244643211365}, 'topk': [{'index': 12, 'folder_name': '2', 'display_name': 'Bien bao cam nguoc chieu', 'confidence': 0.9998244643211365}, {'index': 23, 'folder_name': '8', 'display_name': 'Bien bao giao nhau vong xuyen', 'confidence': 3.6170717066852376e-05}, {'index': 22, 'folder_name': '7', 'display_name': 'Bien bao di thang hoac re trai', 'confidence': 2.019707244471647e-05}, {'index': 14, 'folder_name': '21', 'display_name': 'Bien bao dang bao tri duong bo', 'confidence': 1.9207209334126674e-05}, {'index': 0, 'folder_name': '0', 'display_name': 'Bien bao gioi han toc do 20kmh', 'confidence': 1.2370691365504172e-05}]}\n"
     ]
    }
   ],
   "source": [
    "# API test: YOLOv8 classification model\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# --- User-editable params ---\n",
    "dataset_path = \"D:/2025/Vietnamese-Traffic-Sign/myData\"\n",
    "labels_csv_path = 'D:/2025/Vietnamese-Traffic-Sign/labels.csv'\n",
    "possible_weights = [\n",
    "    'D:/2025/Yolo_Code/runs/yolov8_cls_vi_signs_gpu/weights/best.pt',\n",
    "    'D:/2025/Yolo_Code/runs/yolov8_cls_vi_signs_gpu/weights/last.pt',\n",
    "    'yolov8m-cls.pt',\n",
    "]\n",
    "test_image_path = 'D:/2025/Test_Anh/50.jpg'\n",
    "TOP_K = 5\n",
    "\n",
    "# --- Resolve class names from dataset folder if possible ---\n",
    "if os.path.isdir(dataset_path):\n",
    "    class_names = sorted([d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))])\n",
    "else:\n",
    "    class_names = None\n",
    "\n",
    "print(f\"Dataset path: {dataset_path}\\nFound {len(class_names) if class_names else 0} class folders\")\n",
    "\n",
    "# --- Load labels.csv if available and build mapping ---\n",
    "class_id_to_display = {}\n",
    "if os.path.exists(labels_csv_path):\n",
    "    try:\n",
    "        labels_df = pd.read_csv(labels_csv_path)\n",
    "        print(f\"Loaded labels CSV with columns: {labels_df.columns.tolist()}\")\n",
    "        id_col_candidates = ['ClassID', 'class_id', 'ID', 'id', labels_df.columns[0]]\n",
    "        name_col_candidates = ['Name', 'name', 'Label', 'label', labels_df.columns[1] if labels_df.shape[1] > 1 else labels_df.columns[-1]]\n",
    "        id_col = None\n",
    "        name_col = None\n",
    "        for c in id_col_candidates:\n",
    "            if c in labels_df.columns:\n",
    "                id_col = c\n",
    "                break\n",
    "        for c in name_col_candidates:\n",
    "            if c in labels_df.columns:\n",
    "                name_col = c\n",
    "                break\n",
    "        if id_col is None:\n",
    "            id_col = labels_df.columns[0]\n",
    "        if name_col is None and labels_df.shape[1] > 1:\n",
    "            name_col = labels_df.columns[1]\n",
    "        if name_col is None:\n",
    "            name_col = labels_df.columns[0]\n",
    "        print(f\"Using id column: '{id_col}', name column: '{name_col}'\")\n",
    "        for _, row in labels_df.iterrows():\n",
    "            raw_id = row[id_col]\n",
    "            display = str(row[name_col])\n",
    "            try:\n",
    "                key_int = int(raw_id)\n",
    "                class_id_to_display[key_int] = display\n",
    "            except Exception:\n",
    "                pass\n",
    "            key_str = str(raw_id)\n",
    "            class_id_to_display[key_str] = display\n",
    "        print(f\"Built display mapping for {len(class_id_to_display)} entries (mixed int/str keys)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load/parse labels CSV: {e}\")\n",
    "else:\n",
    "    print(f\"Labels CSV not found at {labels_csv_path} ‚Äî will attempt to use dataset folder names or model.names\")\n",
    "\n",
    "# --- Find a weights file to load ---\n",
    "weights_path = None\n",
    "for p in possible_weights:\n",
    "    if os.path.exists(p):\n",
    "        weights_path = p\n",
    "        break\n",
    "\n",
    "if weights_path is None:\n",
    "    print(\"No trained weights file found in the standard locations. Falling back to 'yolov8m-cls.pt' (downloaded model) if installed.\")\n",
    "    weights_path = 'yolov8m-cls.pt'\n",
    "\n",
    "print(f\"Using weights: {weights_path}\")\n",
    "\n",
    "# --- Load model ---\n",
    "try:\n",
    "    model = YOLO(weights_path)\n",
    "except Exception as e:\n",
    "    print(\"Failed to load model:\", e)\n",
    "    raise\n",
    "\n",
    "# If class names are not available from dataset, try to get from model.names\n",
    "if not class_names:\n",
    "    try:\n",
    "        model_names = model.names\n",
    "        if isinstance(model_names, dict):\n",
    "            class_names = [model_names[i] for i in sorted(model_names.keys())]\n",
    "        else:\n",
    "            class_names = list(model_names)\n",
    "        print(f\"Loaded {len(class_names)} class names from model\")\n",
    "    except Exception:\n",
    "        class_names = None\n",
    "\n",
    "if class_names is None:\n",
    "    print(\"Warning: Could not determine class folder names. Predictions will show index or model names.\")\n",
    "\n",
    "# --- Helper: map predicted index or folder name to display name ---\n",
    "def get_display_name(pred_index:int, folder_name: str=None):\n",
    "    if pred_index in class_id_to_display:\n",
    "        return class_id_to_display[pred_index]\n",
    "    if folder_name is not None:\n",
    "        if folder_name in class_id_to_display:\n",
    "            return class_id_to_display[folder_name]\n",
    "        try:\n",
    "            k = int(folder_name)\n",
    "            if k in class_id_to_display:\n",
    "                return class_id_to_display[k]\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        if hasattr(model, 'names'):\n",
    "            mn = model.names\n",
    "            if isinstance(mn, dict) and pred_index in mn:\n",
    "                return str(mn[pred_index])\n",
    "            elif isinstance(mn, (list, tuple)) and 0 <= pred_index < len(mn):\n",
    "                return str(mn[pred_index])\n",
    "    except Exception:\n",
    "        pass\n",
    "    if folder_name is not None:\n",
    "        return str(folder_name)\n",
    "    return str(pred_index)\n",
    "\n",
    "# --- Check test image ---\n",
    "if not os.path.exists(test_image_path):\n",
    "    raise FileNotFoundError(f\"Test image not found: {test_image_path}\")\n",
    "\n",
    "print(f\"Running inference on: {test_image_path}\")\n",
    "\n",
    "# --- Run inference ---\n",
    "results = model(test_image_path, imgsz=224)  # adjust imgsz if you used different size during train\n",
    "if len(results) == 0:\n",
    "    raise RuntimeError(\"No results returned from model inference\")\n",
    "\n",
    "res = results[0]\n",
    "\n",
    "# Get probs robustly (Ultralytics provides .probs for classification models)\n",
    "probs_raw = getattr(res, 'probs', None)\n",
    "if probs_raw is None:\n",
    "    for alt in ('probs', 'scores', 'conf', 'data'):\n",
    "        probs_raw = getattr(res, alt, None)\n",
    "        if probs_raw is not None:\n",
    "            break\n",
    "\n",
    "if probs_raw is None:\n",
    "    raise RuntimeError('No probabilities found in result object (no .probs/.scores/.conf)')\n",
    "\n",
    "# --- Robust conversion: try many extraction strategies and provide good debug on failure ---\n",
    "try:\n",
    "    import numpy as _np\n",
    "\n",
    "    def _is_scalar_like(x):\n",
    "        return isinstance(x, (float, int, _np.floating, _np.integer))\n",
    "\n",
    "    def to_number(x):\n",
    "        \"\"\"Attempt to extract a single float from x, recursively.\"\"\"\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return float(x.detach().cpu().item())\n",
    "        if _is_scalar_like(x):\n",
    "            return float(x)\n",
    "        if hasattr(x, \"item\") and not callable(x):\n",
    "            try:\n",
    "                return float(x.item())\n",
    "            except Exception:\n",
    "                pass\n",
    "        for attr in (\"tensor\", \"probs\", \"scores\", \"data\", \"values\", \"value\", \"score\", \"prob\", \"confidence\", \"conf\"):\n",
    "            if hasattr(x, attr):\n",
    "                v = getattr(x, attr)\n",
    "                v = v() if callable(v) else v\n",
    "                return to_number(v)\n",
    "        if hasattr(x, \"numpy\"):\n",
    "            try:\n",
    "                arr = x.numpy()\n",
    "                if _is_scalar_like(arr):\n",
    "                    return float(arr)\n",
    "                if hasattr(arr, \"ravel\"):\n",
    "                    flat = arr.ravel()\n",
    "                    if flat.size == 1:\n",
    "                        return float(flat[0])\n",
    "            except Exception:\n",
    "                pass\n",
    "        if hasattr(x, \"tolist\"):\n",
    "            try:\n",
    "                t = x.tolist()\n",
    "                if _is_scalar_like(t):\n",
    "                    return float(t)\n",
    "                if isinstance(t, (list, tuple)) and len(t) == 1:\n",
    "                    return to_number(t[0])\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise ValueError(f\"Cannot convert object of type {type(x)} to a float\")\n",
    "\n",
    "    def convert_to_numpy(obj):\n",
    "        \"\"\"Convert obj into a 1-D numpy array of floats.\"\"\"\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.detach().cpu().numpy()\n",
    "        if isinstance(obj, _np.ndarray):\n",
    "            return obj\n",
    "        if _is_scalar_like(obj):\n",
    "            return _np.asarray([float(obj)])\n",
    "        if hasattr(obj, \"numpy\"):\n",
    "            try:\n",
    "                arr = obj.numpy()\n",
    "                return _np.asarray(arr)\n",
    "            except Exception:\n",
    "                pass\n",
    "        if hasattr(obj, \"tolist\"):\n",
    "            try:\n",
    "                t = obj.tolist()\n",
    "                return convert_to_numpy(t)\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            seq = list(obj)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Object of type {type(obj)} is not iterable and not convertible: {e}\")\n",
    "\n",
    "        vals = []\n",
    "        for i, e in enumerate(seq):\n",
    "            try:\n",
    "                n = to_number(e)\n",
    "                vals.append(n)\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                a = convert_to_numpy(e)\n",
    "                a = _np.asarray(a).ravel()\n",
    "                vals.extend([float(x) for x in a.tolist()])\n",
    "            except Exception as ee:\n",
    "                raise RuntimeError(f\"Failed to extract numeric value from element #{i} of type {type(e)}: {ee}\")\n",
    "\n",
    "        return _np.asarray(vals)\n",
    "\n",
    "    try:\n",
    "        probs = convert_to_numpy(probs_raw)\n",
    "    except Exception as e:\n",
    "        type_info = f\"type(probs_raw)={type(probs_raw)}\"\n",
    "        try:\n",
    "            repr_snip = repr(probs_raw)\n",
    "            if len(repr_snip) > 800:\n",
    "                repr_snip = repr_snip[:800] + '...'\n",
    "        except Exception:\n",
    "            repr_snip = '<unreprable>'\n",
    "        raise RuntimeError(f\"Could not convert probs to numpy array: {e}\\n{type_info}\\nrepr(probs_raw)={repr_snip}\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Could not convert probs to numpy array: {e}')\n",
    "\n",
    "probs = np.asarray(probs)\n",
    "if probs.ndim == 2 and probs.shape[0] == 1:\n",
    "    probs = probs[0]\n",
    "\n",
    "if probs.size == 0:\n",
    "    raise RuntimeError('Probabilities array is empty after conversion')\n",
    "\n",
    "num_classes = probs.shape[0]\n",
    "print(f\"Number of classes (model output): {num_classes}\")\n",
    "\n",
    "# --- Get top-K ---\n",
    "topk = min(TOP_K, num_classes)\n",
    "top_indices = np.argsort(probs)[::-1][:topk]\n",
    "top_probs = probs[top_indices]\n",
    "\n",
    "# Print top-K using display names\n",
    "print('\\nTop predictions:')\n",
    "for rank, (i, p) in enumerate(zip(top_indices, top_probs), start=1):\n",
    "    idx = int(i)\n",
    "    folder_name = None\n",
    "    if class_names and 0 <= idx < len(class_names):\n",
    "        folder_name = class_names[idx]\n",
    "    display_name = get_display_name(idx, folder_name)\n",
    "    print(f\"{rank}. {display_name} (Index {idx}) ‚Äî Confidence: {p:.4f} ({p*100:.2f}%)\")\n",
    "\n",
    "# --- Show image with top-1 display name in title ---\n",
    "img = Image.open(test_image_path).convert('RGB')\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(img)\n",
    "best_idx = int(top_indices[0])\n",
    "best_folder = class_names[best_idx] if class_names and 0 <= best_idx < len(class_names) else None\n",
    "best_name = get_display_name(best_idx, best_folder)\n",
    "best_conf = float(top_probs[0])\n",
    "plt.title(f\"Predicted: {best_name} ‚Äî Confidence: {best_conf:.4f}\", fontsize=12)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Print API-style output\n",
    "prediction = {\n",
    "    'image': os.path.basename(test_image_path),\n",
    "    'top1': {'index': int(best_idx), 'folder_name': best_folder, 'display_name': best_name, 'confidence': best_conf},\n",
    "    'topk': [ {'index': int(i), 'folder_name': (class_names[int(i)] if class_names and 0 <= int(i) < len(class_names) else None), 'display_name': get_display_name(int(i), (class_names[int(i)] if class_names and 0 <= int(i) < len(class_names) else None)), 'confidence': float(p)} for i,p in zip(top_indices, top_probs) ]\n",
    "}\n",
    "\n",
    "print('\\nAPI-style output (dict):')\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bfe3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
